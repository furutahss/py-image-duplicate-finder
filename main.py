import os
import sys
import hashlib
import argparse
from pathlib import Path
from datetime import datetime
from concurrent.futures import ProcessPoolExecutor
from collections import defaultdict

# å¯¾è±¡ã¨ã™ã‚‹æ‹¡å¼µå­
EXTENSIONS = {
    # æ¨™æº–ç”»åƒ
    '.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp', '.tiff',
    # RAWãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
    '.arw',  # SONY
    '.cr2', '.cr3',  # Canon
    '.nef',  # Nikon
    '.orf',  # Olympus
    '.raf',  # Fujifilm
    '.dng',  # Adobe / Digital Negative
    '.rw2'   # Panasonic
}

# ãƒ•ã‚¡ã‚¤ãƒ«ã®MD5ãƒãƒƒã‚·ãƒ¥å€¤ã‚’è¨ˆç®—
# @returns {ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹, ãƒãƒƒã‚·ãƒ¥å€¤}
def calculate_hash(file_path):
    hash_md5 = hashlib.md5()
    try:
        with open(file_path, "rb") as f:
            # 8KBå˜ä½ã§èª­ã¿è¾¼ã¿
            for chunk in iter(lambda: f.read(8192), b""):
                hash_md5.update(chunk)
        return str(file_path), hash_md5.hexdigest()
    except Exception as e:
        return str(file_path), None

# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# @returns  none
def main():
    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°è§£æ
    parser = argparse.ArgumentParser(description="å…¨éšå±¤ã®é‡è¤‡ç”»åƒã‚’ãƒã‚§ãƒƒã‚¯ã—ã¾ã™ã€‚")
    parser.add_argument("dir", help="å¯¾è±¡ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹")
    args = parser.parse_args()

    target_dir = Path(args.dir)
    if not target_dir.is_dir():
        print(f"ã‚¨ãƒ©ãƒ¼: {target_dir} ã¯æœ‰åŠ¹ãªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
        sys.exit(1)

    # 1. å…¨éšå±¤ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒªã‚¹ãƒˆã‚¢ãƒƒãƒ—
    print(f"ğŸ“‚ å…¨éšå±¤ã‚’ã‚¹ã‚­ãƒ£ãƒ³ä¸­: {target_dir}")
    files = [
        p for p in target_dir.rglob('*') 
        if p.is_file()
        and not p.name.startswith('.') 
        and p.suffix.lower() in EXTENSIONS
    ]

    if not files:
        print("å¯¾è±¡ã¨ãªã‚‹ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return

    print(f"ğŸ” {len(files)} æšã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒã‚§ãƒƒã‚¯ä¸­...")

    # 2. ä¸¦åˆ—å‡¦ç†ã§ãƒãƒƒã‚·ãƒ¥è¨ˆç®—
    results = defaultdict(list)
    with ProcessPoolExecutor() as executor:
        for path_str, file_hash in executor.map(calculate_hash, files):
            if file_hash:
                results[file_hash].append(path_str)

    # 3. é‡è¤‡ã®æŠ½å‡º
    duplicates = {h: paths for h, paths in results.items() if len(paths) > 1}

    # 4. çµæœã®æ›¸ãå‡ºã—
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = f"result_{timestamp}.txt"

    with open(output_file, "w", encoding="utf-8") as f:
        f.write(f"--- é‡è¤‡ãƒ•ã‚¡ã‚¤ãƒ«ãƒã‚§ãƒƒã‚¯çµæœ ({datetime.now()}) ---\n")
        f.write(f"å¯¾è±¡ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {target_dir.absolute()}\n")
        f.write(f"ã‚¹ã‚­ãƒ£ãƒ³ç·æ•°: {len(files)}\n\n")

        if not duplicates:
            f.write("é‡è¤‡ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚\n")
        else:
            f.write(f"é‡è¤‡ã‚°ãƒ«ãƒ¼ãƒ—æ•°: {len(duplicates)}\n\n")
            for i, (h, paths) in enumerate(duplicates.items(), 1):
                f.write(f"Group {i} (Hash: {h})\n")
                for path in paths:
                    f.write(f"  - {path}\n")
                f.write("\n")

    print(f"âœ¨ å®Œäº†ï¼çµæœã‚’ {output_file} ã«ä¿å­˜ã—ã¾ã—ãŸã€‚")
    if duplicates:
        print(f"âš ï¸ {len(duplicates)} çµ„ã®é‡è¤‡ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚")

if __name__ == "__main__":
    main()